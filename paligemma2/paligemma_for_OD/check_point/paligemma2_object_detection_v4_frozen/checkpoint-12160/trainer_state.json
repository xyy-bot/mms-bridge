{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 64.0,
  "eval_steps": 500,
  "global_step": 12160,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.2631578947368421,
      "grad_norm": 3.762561559677124,
      "learning_rate": 1.9921039644678404e-05,
      "loss": 2.9246,
      "step": 50
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 8.136635780334473,
      "learning_rate": 1.9838789274551737e-05,
      "loss": 1.7832,
      "step": 100
    },
    {
      "epoch": 0.7894736842105263,
      "grad_norm": 3.734200954437256,
      "learning_rate": 1.975653890442507e-05,
      "loss": 1.7552,
      "step": 150
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 4.538208961486816,
      "learning_rate": 1.9674288534298408e-05,
      "loss": 1.7275,
      "step": 200
    },
    {
      "epoch": 1.3157894736842106,
      "grad_norm": 7.016202449798584,
      "learning_rate": 1.959203816417174e-05,
      "loss": 1.6305,
      "step": 250
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 8.396839141845703,
      "learning_rate": 1.9509787794045075e-05,
      "loss": 1.5689,
      "step": 300
    },
    {
      "epoch": 1.8421052631578947,
      "grad_norm": 9.157722473144531,
      "learning_rate": 1.942753742391841e-05,
      "loss": 1.5578,
      "step": 350
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 5.931095600128174,
      "learning_rate": 1.9345287053791742e-05,
      "loss": 1.4876,
      "step": 400
    },
    {
      "epoch": 2.3684210526315788,
      "grad_norm": 9.736165046691895,
      "learning_rate": 1.9263036683665076e-05,
      "loss": 1.4701,
      "step": 450
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 12.8369140625,
      "learning_rate": 1.9180786313538413e-05,
      "loss": 1.3155,
      "step": 500
    },
    {
      "epoch": 2.8947368421052633,
      "grad_norm": 6.840231418609619,
      "learning_rate": 1.9098535943411747e-05,
      "loss": 1.423,
      "step": 550
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 9.560476303100586,
      "learning_rate": 1.901628557328508e-05,
      "loss": 1.2249,
      "step": 600
    },
    {
      "epoch": 3.4210526315789473,
      "grad_norm": 19.792306900024414,
      "learning_rate": 1.8934035203158417e-05,
      "loss": 1.2961,
      "step": 650
    },
    {
      "epoch": 3.6842105263157894,
      "grad_norm": 18.952491760253906,
      "learning_rate": 1.885178483303175e-05,
      "loss": 1.1778,
      "step": 700
    },
    {
      "epoch": 3.9473684210526314,
      "grad_norm": 9.00761604309082,
      "learning_rate": 1.8769534462905085e-05,
      "loss": 1.1924,
      "step": 750
    },
    {
      "epoch": 4.2105263157894735,
      "grad_norm": 13.013099670410156,
      "learning_rate": 1.8687284092778418e-05,
      "loss": 1.0628,
      "step": 800
    },
    {
      "epoch": 4.473684210526316,
      "grad_norm": 14.32116985321045,
      "learning_rate": 1.8605033722651752e-05,
      "loss": 0.9753,
      "step": 850
    },
    {
      "epoch": 4.7368421052631575,
      "grad_norm": 25.798564910888672,
      "learning_rate": 1.852278335252509e-05,
      "loss": 1.0995,
      "step": 900
    },
    {
      "epoch": 5.0,
      "grad_norm": 15.075006484985352,
      "learning_rate": 1.8440532982398423e-05,
      "loss": 1.0692,
      "step": 950
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 18.518386840820312,
      "learning_rate": 1.8358282612271756e-05,
      "loss": 0.9139,
      "step": 1000
    },
    {
      "epoch": 5.526315789473684,
      "grad_norm": 9.40633487701416,
      "learning_rate": 1.8276032242145093e-05,
      "loss": 0.8745,
      "step": 1050
    },
    {
      "epoch": 5.7894736842105265,
      "grad_norm": 12.500895500183105,
      "learning_rate": 1.8193781872018427e-05,
      "loss": 0.9678,
      "step": 1100
    },
    {
      "epoch": 6.052631578947368,
      "grad_norm": 8.686744689941406,
      "learning_rate": 1.811153150189176e-05,
      "loss": 0.8536,
      "step": 1150
    },
    {
      "epoch": 6.315789473684211,
      "grad_norm": 17.413928985595703,
      "learning_rate": 1.8029281131765094e-05,
      "loss": 0.7136,
      "step": 1200
    },
    {
      "epoch": 6.578947368421053,
      "grad_norm": 12.207537651062012,
      "learning_rate": 1.7947030761638428e-05,
      "loss": 0.7156,
      "step": 1250
    },
    {
      "epoch": 6.842105263157895,
      "grad_norm": 17.10335922241211,
      "learning_rate": 1.786478039151176e-05,
      "loss": 0.7824,
      "step": 1300
    },
    {
      "epoch": 7.105263157894737,
      "grad_norm": 15.943507194519043,
      "learning_rate": 1.77825300213851e-05,
      "loss": 0.7879,
      "step": 1350
    },
    {
      "epoch": 7.368421052631579,
      "grad_norm": 11.76269245147705,
      "learning_rate": 1.7700279651258432e-05,
      "loss": 0.6199,
      "step": 1400
    },
    {
      "epoch": 7.631578947368421,
      "grad_norm": 11.5512113571167,
      "learning_rate": 1.7618029281131766e-05,
      "loss": 0.6864,
      "step": 1450
    },
    {
      "epoch": 7.894736842105263,
      "grad_norm": 14.703217506408691,
      "learning_rate": 1.7535778911005103e-05,
      "loss": 0.674,
      "step": 1500
    },
    {
      "epoch": 8.157894736842104,
      "grad_norm": 12.537049293518066,
      "learning_rate": 1.7453528540878436e-05,
      "loss": 0.6021,
      "step": 1550
    },
    {
      "epoch": 8.421052631578947,
      "grad_norm": 21.437965393066406,
      "learning_rate": 1.737127817075177e-05,
      "loss": 0.5215,
      "step": 1600
    },
    {
      "epoch": 8.68421052631579,
      "grad_norm": 15.459246635437012,
      "learning_rate": 1.7289027800625104e-05,
      "loss": 0.5683,
      "step": 1650
    },
    {
      "epoch": 8.947368421052632,
      "grad_norm": 12.387853622436523,
      "learning_rate": 1.7206777430498437e-05,
      "loss": 0.5971,
      "step": 1700
    },
    {
      "epoch": 9.210526315789474,
      "grad_norm": 8.141304969787598,
      "learning_rate": 1.7124527060371774e-05,
      "loss": 0.4808,
      "step": 1750
    },
    {
      "epoch": 9.473684210526315,
      "grad_norm": 8.875887870788574,
      "learning_rate": 1.7042276690245108e-05,
      "loss": 0.4165,
      "step": 1800
    },
    {
      "epoch": 9.736842105263158,
      "grad_norm": 25.774208068847656,
      "learning_rate": 1.696002632011844e-05,
      "loss": 0.4789,
      "step": 1850
    },
    {
      "epoch": 10.0,
      "grad_norm": 21.013755798339844,
      "learning_rate": 1.687777594999178e-05,
      "loss": 0.4766,
      "step": 1900
    },
    {
      "epoch": 10.263157894736842,
      "grad_norm": 13.496308326721191,
      "learning_rate": 1.6795525579865112e-05,
      "loss": 0.3341,
      "step": 1950
    },
    {
      "epoch": 10.526315789473685,
      "grad_norm": 12.593976974487305,
      "learning_rate": 1.6713275209738446e-05,
      "loss": 0.4005,
      "step": 2000
    },
    {
      "epoch": 10.789473684210526,
      "grad_norm": 13.690397262573242,
      "learning_rate": 1.663102483961178e-05,
      "loss": 0.3826,
      "step": 2050
    },
    {
      "epoch": 11.052631578947368,
      "grad_norm": 11.260043144226074,
      "learning_rate": 1.6548774469485113e-05,
      "loss": 0.3671,
      "step": 2100
    },
    {
      "epoch": 11.31578947368421,
      "grad_norm": 7.143436431884766,
      "learning_rate": 1.6466524099358447e-05,
      "loss": 0.3111,
      "step": 2150
    },
    {
      "epoch": 11.578947368421053,
      "grad_norm": 17.1535701751709,
      "learning_rate": 1.6384273729231784e-05,
      "loss": 0.2777,
      "step": 2200
    },
    {
      "epoch": 11.842105263157894,
      "grad_norm": 3.5596044063568115,
      "learning_rate": 1.6302023359105117e-05,
      "loss": 0.3395,
      "step": 2250
    },
    {
      "epoch": 12.105263157894736,
      "grad_norm": 54.263248443603516,
      "learning_rate": 1.6219772988978454e-05,
      "loss": 0.286,
      "step": 2300
    },
    {
      "epoch": 12.368421052631579,
      "grad_norm": 0.8808864951133728,
      "learning_rate": 1.6137522618851788e-05,
      "loss": 0.2144,
      "step": 2350
    },
    {
      "epoch": 12.631578947368421,
      "grad_norm": 17.87449836730957,
      "learning_rate": 1.605527224872512e-05,
      "loss": 0.2553,
      "step": 2400
    },
    {
      "epoch": 12.894736842105264,
      "grad_norm": 27.598709106445312,
      "learning_rate": 1.5973021878598455e-05,
      "loss": 0.2884,
      "step": 2450
    },
    {
      "epoch": 13.157894736842104,
      "grad_norm": 44.452430725097656,
      "learning_rate": 1.589077150847179e-05,
      "loss": 0.2727,
      "step": 2500
    },
    {
      "epoch": 13.421052631578947,
      "grad_norm": 0.8431589603424072,
      "learning_rate": 1.5808521138345122e-05,
      "loss": 0.1789,
      "step": 2550
    },
    {
      "epoch": 13.68421052631579,
      "grad_norm": 16.08515739440918,
      "learning_rate": 1.572627076821846e-05,
      "loss": 0.1834,
      "step": 2600
    },
    {
      "epoch": 13.947368421052632,
      "grad_norm": 14.152538299560547,
      "learning_rate": 1.5644020398091793e-05,
      "loss": 0.2226,
      "step": 2650
    },
    {
      "epoch": 14.210526315789474,
      "grad_norm": 27.324970245361328,
      "learning_rate": 1.5561770027965127e-05,
      "loss": 0.21,
      "step": 2700
    },
    {
      "epoch": 14.473684210526315,
      "grad_norm": 20.43934440612793,
      "learning_rate": 1.5479519657838464e-05,
      "loss": 0.1464,
      "step": 2750
    },
    {
      "epoch": 14.736842105263158,
      "grad_norm": 4.054808139801025,
      "learning_rate": 1.5397269287711797e-05,
      "loss": 0.189,
      "step": 2800
    },
    {
      "epoch": 15.0,
      "grad_norm": 6.116767406463623,
      "learning_rate": 1.531501891758513e-05,
      "loss": 0.1954,
      "step": 2850
    },
    {
      "epoch": 15.263157894736842,
      "grad_norm": 17.732891082763672,
      "learning_rate": 1.5232768547458465e-05,
      "loss": 0.1359,
      "step": 2900
    },
    {
      "epoch": 15.526315789473685,
      "grad_norm": 2.6345105171203613,
      "learning_rate": 1.5150518177331798e-05,
      "loss": 0.151,
      "step": 2950
    },
    {
      "epoch": 15.789473684210526,
      "grad_norm": 0.3583073318004608,
      "learning_rate": 1.5068267807205135e-05,
      "loss": 0.1066,
      "step": 3000
    },
    {
      "epoch": 16.05263157894737,
      "grad_norm": 12.196622848510742,
      "learning_rate": 1.4986017437078469e-05,
      "loss": 0.1082,
      "step": 3050
    },
    {
      "epoch": 16.31578947368421,
      "grad_norm": 0.13032732903957367,
      "learning_rate": 1.4903767066951803e-05,
      "loss": 0.1061,
      "step": 3100
    },
    {
      "epoch": 16.57894736842105,
      "grad_norm": 17.25040626525879,
      "learning_rate": 1.4821516696825138e-05,
      "loss": 0.1038,
      "step": 3150
    },
    {
      "epoch": 16.842105263157894,
      "grad_norm": 13.972707748413086,
      "learning_rate": 1.4739266326698472e-05,
      "loss": 0.1408,
      "step": 3200
    },
    {
      "epoch": 17.105263157894736,
      "grad_norm": 24.749256134033203,
      "learning_rate": 1.4657015956571805e-05,
      "loss": 0.1235,
      "step": 3250
    },
    {
      "epoch": 17.36842105263158,
      "grad_norm": 0.3026176393032074,
      "learning_rate": 1.457476558644514e-05,
      "loss": 0.0798,
      "step": 3300
    },
    {
      "epoch": 17.63157894736842,
      "grad_norm": 0.2030540108680725,
      "learning_rate": 1.4492515216318474e-05,
      "loss": 0.1192,
      "step": 3350
    },
    {
      "epoch": 17.894736842105264,
      "grad_norm": 0.49142250418663025,
      "learning_rate": 1.4410264846191808e-05,
      "loss": 0.122,
      "step": 3400
    },
    {
      "epoch": 18.157894736842106,
      "grad_norm": 2.093228578567505,
      "learning_rate": 1.4328014476065145e-05,
      "loss": 0.0748,
      "step": 3450
    },
    {
      "epoch": 18.42105263157895,
      "grad_norm": 2.2317922115325928,
      "learning_rate": 1.4245764105938478e-05,
      "loss": 0.0874,
      "step": 3500
    },
    {
      "epoch": 18.68421052631579,
      "grad_norm": 0.3960798382759094,
      "learning_rate": 1.4163513735811812e-05,
      "loss": 0.1041,
      "step": 3550
    },
    {
      "epoch": 18.94736842105263,
      "grad_norm": 24.473140716552734,
      "learning_rate": 1.4081263365685147e-05,
      "loss": 0.0976,
      "step": 3600
    },
    {
      "epoch": 19.210526315789473,
      "grad_norm": 0.29065024852752686,
      "learning_rate": 1.3999012995558481e-05,
      "loss": 0.07,
      "step": 3650
    },
    {
      "epoch": 19.473684210526315,
      "grad_norm": 3.950845718383789,
      "learning_rate": 1.3916762625431816e-05,
      "loss": 0.0827,
      "step": 3700
    },
    {
      "epoch": 19.736842105263158,
      "grad_norm": 0.09856250137090683,
      "learning_rate": 1.383451225530515e-05,
      "loss": 0.0831,
      "step": 3750
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.08677415549755096,
      "learning_rate": 1.3752261885178484e-05,
      "loss": 0.0671,
      "step": 3800
    },
    {
      "epoch": 20.263157894736842,
      "grad_norm": 0.07836998999118805,
      "learning_rate": 1.367001151505182e-05,
      "loss": 0.058,
      "step": 3850
    },
    {
      "epoch": 20.526315789473685,
      "grad_norm": 0.1215454488992691,
      "learning_rate": 1.3587761144925154e-05,
      "loss": 0.1089,
      "step": 3900
    },
    {
      "epoch": 20.789473684210527,
      "grad_norm": 13.440247535705566,
      "learning_rate": 1.3505510774798488e-05,
      "loss": 0.0721,
      "step": 3950
    },
    {
      "epoch": 21.05263157894737,
      "grad_norm": 10.13041877746582,
      "learning_rate": 1.3423260404671823e-05,
      "loss": 0.0653,
      "step": 4000
    },
    {
      "epoch": 21.31578947368421,
      "grad_norm": 0.04857300966978073,
      "learning_rate": 1.3341010034545157e-05,
      "loss": 0.0586,
      "step": 4050
    },
    {
      "epoch": 21.57894736842105,
      "grad_norm": 0.16455146670341492,
      "learning_rate": 1.325875966441849e-05,
      "loss": 0.0472,
      "step": 4100
    },
    {
      "epoch": 21.842105263157894,
      "grad_norm": 0.08113507181406021,
      "learning_rate": 1.3176509294291826e-05,
      "loss": 0.0991,
      "step": 4150
    },
    {
      "epoch": 22.105263157894736,
      "grad_norm": 14.112418174743652,
      "learning_rate": 1.309425892416516e-05,
      "loss": 0.1029,
      "step": 4200
    },
    {
      "epoch": 22.36842105263158,
      "grad_norm": 8.165518760681152,
      "learning_rate": 1.3012008554038493e-05,
      "loss": 0.0792,
      "step": 4250
    },
    {
      "epoch": 22.63157894736842,
      "grad_norm": 16.419757843017578,
      "learning_rate": 1.2929758183911828e-05,
      "loss": 0.0497,
      "step": 4300
    },
    {
      "epoch": 22.894736842105264,
      "grad_norm": 0.13506099581718445,
      "learning_rate": 1.2847507813785162e-05,
      "loss": 0.063,
      "step": 4350
    },
    {
      "epoch": 23.157894736842106,
      "grad_norm": 1.0818651914596558,
      "learning_rate": 1.2765257443658499e-05,
      "loss": 0.0662,
      "step": 4400
    },
    {
      "epoch": 23.42105263157895,
      "grad_norm": 0.12555360794067383,
      "learning_rate": 1.2683007073531833e-05,
      "loss": 0.0648,
      "step": 4450
    },
    {
      "epoch": 23.68421052631579,
      "grad_norm": 4.869593620300293,
      "learning_rate": 1.2600756703405166e-05,
      "loss": 0.0876,
      "step": 4500
    },
    {
      "epoch": 23.94736842105263,
      "grad_norm": 16.837839126586914,
      "learning_rate": 1.2518506333278502e-05,
      "loss": 0.0888,
      "step": 4550
    },
    {
      "epoch": 24.210526315789473,
      "grad_norm": 0.26438137888908386,
      "learning_rate": 1.2436255963151835e-05,
      "loss": 0.0539,
      "step": 4600
    },
    {
      "epoch": 24.473684210526315,
      "grad_norm": 0.05488712340593338,
      "learning_rate": 1.2354005593025169e-05,
      "loss": 0.0565,
      "step": 4650
    },
    {
      "epoch": 24.736842105263158,
      "grad_norm": 0.9383078813552856,
      "learning_rate": 1.2271755222898504e-05,
      "loss": 0.0685,
      "step": 4700
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.14994406700134277,
      "learning_rate": 1.2189504852771838e-05,
      "loss": 0.117,
      "step": 4750
    },
    {
      "epoch": 25.263157894736842,
      "grad_norm": 15.908747673034668,
      "learning_rate": 1.2107254482645171e-05,
      "loss": 0.0535,
      "step": 4800
    },
    {
      "epoch": 25.526315789473685,
      "grad_norm": 0.029631048440933228,
      "learning_rate": 1.2025004112518508e-05,
      "loss": 0.1015,
      "step": 4850
    },
    {
      "epoch": 25.789473684210527,
      "grad_norm": 5.094506740570068,
      "learning_rate": 1.1942753742391842e-05,
      "loss": 0.0656,
      "step": 4900
    },
    {
      "epoch": 26.05263157894737,
      "grad_norm": 0.026259901002049446,
      "learning_rate": 1.1860503372265176e-05,
      "loss": 0.0588,
      "step": 4950
    },
    {
      "epoch": 26.31578947368421,
      "grad_norm": 12.268563270568848,
      "learning_rate": 1.1778253002138511e-05,
      "loss": 0.0636,
      "step": 5000
    },
    {
      "epoch": 26.57894736842105,
      "grad_norm": 0.2320307493209839,
      "learning_rate": 1.1696002632011845e-05,
      "loss": 0.0516,
      "step": 5050
    },
    {
      "epoch": 26.842105263157894,
      "grad_norm": 30.959537506103516,
      "learning_rate": 1.1613752261885178e-05,
      "loss": 0.0423,
      "step": 5100
    },
    {
      "epoch": 27.105263157894736,
      "grad_norm": 0.04474784806370735,
      "learning_rate": 1.1531501891758514e-05,
      "loss": 0.092,
      "step": 5150
    },
    {
      "epoch": 27.36842105263158,
      "grad_norm": 16.613529205322266,
      "learning_rate": 1.1449251521631847e-05,
      "loss": 0.0586,
      "step": 5200
    },
    {
      "epoch": 27.63157894736842,
      "grad_norm": 0.10745732486248016,
      "learning_rate": 1.1367001151505184e-05,
      "loss": 0.0726,
      "step": 5250
    },
    {
      "epoch": 27.894736842105264,
      "grad_norm": 11.184694290161133,
      "learning_rate": 1.1284750781378518e-05,
      "loss": 0.0536,
      "step": 5300
    },
    {
      "epoch": 28.157894736842106,
      "grad_norm": 22.16130256652832,
      "learning_rate": 1.1202500411251851e-05,
      "loss": 0.0694,
      "step": 5350
    },
    {
      "epoch": 28.42105263157895,
      "grad_norm": 0.04576314240694046,
      "learning_rate": 1.1120250041125187e-05,
      "loss": 0.0609,
      "step": 5400
    },
    {
      "epoch": 28.68421052631579,
      "grad_norm": 0.04690255597233772,
      "learning_rate": 1.103799967099852e-05,
      "loss": 0.0686,
      "step": 5450
    },
    {
      "epoch": 28.94736842105263,
      "grad_norm": 0.169905886054039,
      "learning_rate": 1.0955749300871854e-05,
      "loss": 0.0584,
      "step": 5500
    },
    {
      "epoch": 29.210526315789473,
      "grad_norm": 12.76207160949707,
      "learning_rate": 1.087349893074519e-05,
      "loss": 0.0788,
      "step": 5550
    },
    {
      "epoch": 29.473684210526315,
      "grad_norm": 0.05617891624569893,
      "learning_rate": 1.0791248560618523e-05,
      "loss": 0.031,
      "step": 5600
    },
    {
      "epoch": 29.736842105263158,
      "grad_norm": 11.144431114196777,
      "learning_rate": 1.0708998190491857e-05,
      "loss": 0.0632,
      "step": 5650
    },
    {
      "epoch": 30.0,
      "grad_norm": 14.057856559753418,
      "learning_rate": 1.0626747820365194e-05,
      "loss": 0.064,
      "step": 5700
    },
    {
      "epoch": 30.263157894736842,
      "grad_norm": 0.08932117372751236,
      "learning_rate": 1.0544497450238527e-05,
      "loss": 0.0506,
      "step": 5750
    },
    {
      "epoch": 30.526315789473685,
      "grad_norm": 2.0227701663970947,
      "learning_rate": 1.0462247080111861e-05,
      "loss": 0.0404,
      "step": 5800
    },
    {
      "epoch": 30.789473684210527,
      "grad_norm": 15.582093238830566,
      "learning_rate": 1.0379996709985196e-05,
      "loss": 0.097,
      "step": 5850
    },
    {
      "epoch": 31.05263157894737,
      "grad_norm": 0.02593936212360859,
      "learning_rate": 1.029774633985853e-05,
      "loss": 0.0217,
      "step": 5900
    },
    {
      "epoch": 31.31578947368421,
      "grad_norm": 0.043358903378248215,
      "learning_rate": 1.0215495969731865e-05,
      "loss": 0.0681,
      "step": 5950
    },
    {
      "epoch": 31.57894736842105,
      "grad_norm": 0.08038466423749924,
      "learning_rate": 1.0133245599605199e-05,
      "loss": 0.0877,
      "step": 6000
    },
    {
      "epoch": 31.842105263157894,
      "grad_norm": 24.85272979736328,
      "learning_rate": 1.0050995229478532e-05,
      "loss": 0.04,
      "step": 6050
    },
    {
      "epoch": 32.10526315789474,
      "grad_norm": 0.06649663299322128,
      "learning_rate": 9.968744859351868e-06,
      "loss": 0.0532,
      "step": 6100
    },
    {
      "epoch": 32.36842105263158,
      "grad_norm": 0.046455156058073044,
      "learning_rate": 9.886494489225203e-06,
      "loss": 0.0519,
      "step": 6150
    },
    {
      "epoch": 32.63157894736842,
      "grad_norm": 7.732378005981445,
      "learning_rate": 9.804244119098537e-06,
      "loss": 0.0487,
      "step": 6200
    },
    {
      "epoch": 32.89473684210526,
      "grad_norm": 12.584457397460938,
      "learning_rate": 9.72199374897187e-06,
      "loss": 0.0561,
      "step": 6250
    },
    {
      "epoch": 33.1578947368421,
      "grad_norm": 0.9393691420555115,
      "learning_rate": 9.639743378845206e-06,
      "loss": 0.0698,
      "step": 6300
    },
    {
      "epoch": 33.421052631578945,
      "grad_norm": 0.03542657569050789,
      "learning_rate": 9.557493008718541e-06,
      "loss": 0.0605,
      "step": 6350
    },
    {
      "epoch": 33.68421052631579,
      "grad_norm": 4.780294418334961,
      "learning_rate": 9.475242638591875e-06,
      "loss": 0.0434,
      "step": 6400
    },
    {
      "epoch": 33.94736842105263,
      "grad_norm": 3.37921404838562,
      "learning_rate": 9.392992268465208e-06,
      "loss": 0.0722,
      "step": 6450
    },
    {
      "epoch": 34.21052631578947,
      "grad_norm": 0.03972186520695686,
      "learning_rate": 9.310741898338544e-06,
      "loss": 0.0574,
      "step": 6500
    },
    {
      "epoch": 34.473684210526315,
      "grad_norm": 0.05324997752904892,
      "learning_rate": 9.228491528211879e-06,
      "loss": 0.0526,
      "step": 6550
    },
    {
      "epoch": 34.73684210526316,
      "grad_norm": 6.936676502227783,
      "learning_rate": 9.146241158085213e-06,
      "loss": 0.0587,
      "step": 6600
    },
    {
      "epoch": 35.0,
      "grad_norm": 12.88022232055664,
      "learning_rate": 9.063990787958546e-06,
      "loss": 0.0603,
      "step": 6650
    },
    {
      "epoch": 35.26315789473684,
      "grad_norm": 0.02383059449493885,
      "learning_rate": 8.981740417831882e-06,
      "loss": 0.0477,
      "step": 6700
    },
    {
      "epoch": 35.526315789473685,
      "grad_norm": 0.043980490416288376,
      "learning_rate": 8.899490047705215e-06,
      "loss": 0.0404,
      "step": 6750
    },
    {
      "epoch": 35.78947368421053,
      "grad_norm": 0.021283045411109924,
      "learning_rate": 8.817239677578549e-06,
      "loss": 0.052,
      "step": 6800
    },
    {
      "epoch": 36.05263157894737,
      "grad_norm": 0.027094362303614616,
      "learning_rate": 8.734989307451884e-06,
      "loss": 0.0299,
      "step": 6850
    },
    {
      "epoch": 36.31578947368421,
      "grad_norm": 0.01734965853393078,
      "learning_rate": 8.65273893732522e-06,
      "loss": 0.031,
      "step": 6900
    },
    {
      "epoch": 36.578947368421055,
      "grad_norm": 16.766857147216797,
      "learning_rate": 8.570488567198553e-06,
      "loss": 0.0638,
      "step": 6950
    },
    {
      "epoch": 36.8421052631579,
      "grad_norm": 0.07115339487791061,
      "learning_rate": 8.488238197071887e-06,
      "loss": 0.0509,
      "step": 7000
    },
    {
      "epoch": 37.10526315789474,
      "grad_norm": 5.819223880767822,
      "learning_rate": 8.405987826945222e-06,
      "loss": 0.0674,
      "step": 7050
    },
    {
      "epoch": 37.36842105263158,
      "grad_norm": 0.19009369611740112,
      "learning_rate": 8.323737456818556e-06,
      "loss": 0.0575,
      "step": 7100
    },
    {
      "epoch": 37.63157894736842,
      "grad_norm": 0.13895303010940552,
      "learning_rate": 8.241487086691891e-06,
      "loss": 0.0356,
      "step": 7150
    },
    {
      "epoch": 37.89473684210526,
      "grad_norm": 0.10203073918819427,
      "learning_rate": 8.159236716565225e-06,
      "loss": 0.0333,
      "step": 7200
    },
    {
      "epoch": 38.1578947368421,
      "grad_norm": 0.020018195733428,
      "learning_rate": 8.07698634643856e-06,
      "loss": 0.07,
      "step": 7250
    },
    {
      "epoch": 38.421052631578945,
      "grad_norm": 0.11508628726005554,
      "learning_rate": 7.994735976311894e-06,
      "loss": 0.0269,
      "step": 7300
    },
    {
      "epoch": 38.68421052631579,
      "grad_norm": 0.027000034227967262,
      "learning_rate": 7.912485606185229e-06,
      "loss": 0.0223,
      "step": 7350
    },
    {
      "epoch": 38.94736842105263,
      "grad_norm": 0.030918708071112633,
      "learning_rate": 7.830235236058563e-06,
      "loss": 0.0486,
      "step": 7400
    },
    {
      "epoch": 39.21052631578947,
      "grad_norm": 0.012270065024495125,
      "learning_rate": 7.747984865931896e-06,
      "loss": 0.0662,
      "step": 7450
    },
    {
      "epoch": 39.473684210526315,
      "grad_norm": 0.8717865943908691,
      "learning_rate": 7.665734495805231e-06,
      "loss": 0.0587,
      "step": 7500
    },
    {
      "epoch": 39.73684210526316,
      "grad_norm": 0.13986799120903015,
      "learning_rate": 7.583484125678566e-06,
      "loss": 0.0544,
      "step": 7550
    },
    {
      "epoch": 40.0,
      "grad_norm": 5.259396553039551,
      "learning_rate": 7.501233755551901e-06,
      "loss": 0.0716,
      "step": 7600
    },
    {
      "epoch": 40.26315789473684,
      "grad_norm": 8.33730411529541,
      "learning_rate": 7.418983385425235e-06,
      "loss": 0.0507,
      "step": 7650
    },
    {
      "epoch": 40.526315789473685,
      "grad_norm": 0.048007767647504807,
      "learning_rate": 7.336733015298569e-06,
      "loss": 0.0406,
      "step": 7700
    },
    {
      "epoch": 40.78947368421053,
      "grad_norm": 0.056119441986083984,
      "learning_rate": 7.254482645171904e-06,
      "loss": 0.0503,
      "step": 7750
    },
    {
      "epoch": 41.05263157894737,
      "grad_norm": 0.01638668030500412,
      "learning_rate": 7.1722322750452375e-06,
      "loss": 0.0606,
      "step": 7800
    },
    {
      "epoch": 41.31578947368421,
      "grad_norm": 4.123475551605225,
      "learning_rate": 7.089981904918573e-06,
      "loss": 0.0395,
      "step": 7850
    },
    {
      "epoch": 41.578947368421055,
      "grad_norm": 0.04119294881820679,
      "learning_rate": 7.007731534791907e-06,
      "loss": 0.0413,
      "step": 7900
    },
    {
      "epoch": 41.8421052631579,
      "grad_norm": 0.04448239877820015,
      "learning_rate": 6.925481164665242e-06,
      "loss": 0.0334,
      "step": 7950
    },
    {
      "epoch": 42.10526315789474,
      "grad_norm": 5.641326904296875,
      "learning_rate": 6.843230794538575e-06,
      "loss": 0.07,
      "step": 8000
    },
    {
      "epoch": 42.36842105263158,
      "grad_norm": 11.503450393676758,
      "learning_rate": 6.760980424411911e-06,
      "loss": 0.0339,
      "step": 8050
    },
    {
      "epoch": 42.63157894736842,
      "grad_norm": 24.06621742248535,
      "learning_rate": 6.678730054285245e-06,
      "loss": 0.0674,
      "step": 8100
    },
    {
      "epoch": 42.89473684210526,
      "grad_norm": 0.10800109058618546,
      "learning_rate": 6.596479684158579e-06,
      "loss": 0.0497,
      "step": 8150
    },
    {
      "epoch": 43.1578947368421,
      "grad_norm": 0.32823121547698975,
      "learning_rate": 6.514229314031913e-06,
      "loss": 0.052,
      "step": 8200
    },
    {
      "epoch": 43.421052631578945,
      "grad_norm": 9.262369155883789,
      "learning_rate": 6.431978943905249e-06,
      "loss": 0.0676,
      "step": 8250
    },
    {
      "epoch": 43.68421052631579,
      "grad_norm": 21.876737594604492,
      "learning_rate": 6.349728573778583e-06,
      "loss": 0.0392,
      "step": 8300
    },
    {
      "epoch": 43.94736842105263,
      "grad_norm": 0.024302326142787933,
      "learning_rate": 6.267478203651917e-06,
      "loss": 0.0429,
      "step": 8350
    },
    {
      "epoch": 44.21052631578947,
      "grad_norm": 0.008170894347131252,
      "learning_rate": 6.185227833525251e-06,
      "loss": 0.0653,
      "step": 8400
    },
    {
      "epoch": 44.473684210526315,
      "grad_norm": 8.351838111877441,
      "learning_rate": 6.102977463398586e-06,
      "loss": 0.0418,
      "step": 8450
    },
    {
      "epoch": 44.73684210526316,
      "grad_norm": 15.842894554138184,
      "learning_rate": 6.020727093271919e-06,
      "loss": 0.0439,
      "step": 8500
    },
    {
      "epoch": 45.0,
      "grad_norm": 0.030777841806411743,
      "learning_rate": 5.938476723145255e-06,
      "loss": 0.0403,
      "step": 8550
    },
    {
      "epoch": 45.26315789473684,
      "grad_norm": 0.029339974746108055,
      "learning_rate": 5.856226353018589e-06,
      "loss": 0.0497,
      "step": 8600
    },
    {
      "epoch": 45.526315789473685,
      "grad_norm": 5.482687473297119,
      "learning_rate": 5.773975982891924e-06,
      "loss": 0.0224,
      "step": 8650
    },
    {
      "epoch": 45.78947368421053,
      "grad_norm": 4.5402445793151855,
      "learning_rate": 5.691725612765257e-06,
      "loss": 0.0237,
      "step": 8700
    },
    {
      "epoch": 46.05263157894737,
      "grad_norm": 0.26700282096862793,
      "learning_rate": 5.6094752426385926e-06,
      "loss": 0.0543,
      "step": 8750
    },
    {
      "epoch": 46.31578947368421,
      "grad_norm": 2.270230293273926,
      "learning_rate": 5.527224872511927e-06,
      "loss": 0.0634,
      "step": 8800
    },
    {
      "epoch": 46.578947368421055,
      "grad_norm": 0.02310815639793873,
      "learning_rate": 5.444974502385261e-06,
      "loss": 0.0587,
      "step": 8850
    },
    {
      "epoch": 46.8421052631579,
      "grad_norm": 0.020672880113124847,
      "learning_rate": 5.362724132258595e-06,
      "loss": 0.0365,
      "step": 8900
    },
    {
      "epoch": 47.10526315789474,
      "grad_norm": 0.4657648205757141,
      "learning_rate": 5.2804737621319305e-06,
      "loss": 0.0386,
      "step": 8950
    },
    {
      "epoch": 47.36842105263158,
      "grad_norm": 17.36696434020996,
      "learning_rate": 5.198223392005265e-06,
      "loss": 0.0507,
      "step": 9000
    },
    {
      "epoch": 47.63157894736842,
      "grad_norm": 2.3989059925079346,
      "learning_rate": 5.1159730218785986e-06,
      "loss": 0.0481,
      "step": 9050
    },
    {
      "epoch": 47.89473684210526,
      "grad_norm": 12.152365684509277,
      "learning_rate": 5.033722651751933e-06,
      "loss": 0.0466,
      "step": 9100
    },
    {
      "epoch": 48.1578947368421,
      "grad_norm": 5.840763092041016,
      "learning_rate": 4.9514722816252675e-06,
      "loss": 0.044,
      "step": 9150
    },
    {
      "epoch": 48.421052631578945,
      "grad_norm": 0.0553225576877594,
      "learning_rate": 4.869221911498602e-06,
      "loss": 0.0388,
      "step": 9200
    },
    {
      "epoch": 48.68421052631579,
      "grad_norm": 11.150566101074219,
      "learning_rate": 4.7869715413719365e-06,
      "loss": 0.0364,
      "step": 9250
    },
    {
      "epoch": 48.94736842105263,
      "grad_norm": 1.500546932220459,
      "learning_rate": 4.704721171245271e-06,
      "loss": 0.0393,
      "step": 9300
    },
    {
      "epoch": 49.21052631578947,
      "grad_norm": 0.026386534795165062,
      "learning_rate": 4.6224708011186054e-06,
      "loss": 0.052,
      "step": 9350
    },
    {
      "epoch": 49.473684210526315,
      "grad_norm": 0.15525545179843903,
      "learning_rate": 4.54022043099194e-06,
      "loss": 0.0399,
      "step": 9400
    },
    {
      "epoch": 49.73684210526316,
      "grad_norm": 0.01812112331390381,
      "learning_rate": 4.457970060865274e-06,
      "loss": 0.035,
      "step": 9450
    },
    {
      "epoch": 50.0,
      "grad_norm": 0.030128827318549156,
      "learning_rate": 4.375719690738609e-06,
      "loss": 0.0282,
      "step": 9500
    },
    {
      "epoch": 50.26315789473684,
      "grad_norm": 0.01965998485684395,
      "learning_rate": 4.293469320611943e-06,
      "loss": 0.0304,
      "step": 9550
    },
    {
      "epoch": 50.526315789473685,
      "grad_norm": 0.019291942939162254,
      "learning_rate": 4.211218950485278e-06,
      "loss": 0.0592,
      "step": 9600
    },
    {
      "epoch": 50.78947368421053,
      "grad_norm": 0.01603982225060463,
      "learning_rate": 4.128968580358612e-06,
      "loss": 0.0398,
      "step": 9650
    },
    {
      "epoch": 51.05263157894737,
      "grad_norm": 3.8761661052703857,
      "learning_rate": 4.046718210231946e-06,
      "loss": 0.0333,
      "step": 9700
    },
    {
      "epoch": 51.31578947368421,
      "grad_norm": 14.387907981872559,
      "learning_rate": 3.964467840105281e-06,
      "loss": 0.0565,
      "step": 9750
    },
    {
      "epoch": 51.578947368421055,
      "grad_norm": 0.07020875811576843,
      "learning_rate": 3.882217469978615e-06,
      "loss": 0.0551,
      "step": 9800
    },
    {
      "epoch": 51.8421052631579,
      "grad_norm": 0.01738949492573738,
      "learning_rate": 3.7999670998519494e-06,
      "loss": 0.0486,
      "step": 9850
    },
    {
      "epoch": 52.10526315789474,
      "grad_norm": 0.022119909524917603,
      "learning_rate": 3.7177167297252843e-06,
      "loss": 0.0431,
      "step": 9900
    },
    {
      "epoch": 52.36842105263158,
      "grad_norm": 0.02076862007379532,
      "learning_rate": 3.6354663595986183e-06,
      "loss": 0.0337,
      "step": 9950
    },
    {
      "epoch": 52.63157894736842,
      "grad_norm": 0.03298547863960266,
      "learning_rate": 3.553215989471953e-06,
      "loss": 0.0439,
      "step": 10000
    },
    {
      "epoch": 52.89473684210526,
      "grad_norm": 1.7316023111343384,
      "learning_rate": 3.4709656193452873e-06,
      "loss": 0.0286,
      "step": 10050
    },
    {
      "epoch": 53.1578947368421,
      "grad_norm": 0.01596706174314022,
      "learning_rate": 3.3887152492186213e-06,
      "loss": 0.0261,
      "step": 10100
    },
    {
      "epoch": 53.421052631578945,
      "grad_norm": 2.089139699935913,
      "learning_rate": 3.3064648790919562e-06,
      "loss": 0.0438,
      "step": 10150
    },
    {
      "epoch": 53.68421052631579,
      "grad_norm": 5.213767051696777,
      "learning_rate": 3.2242145089652903e-06,
      "loss": 0.06,
      "step": 10200
    },
    {
      "epoch": 53.94736842105263,
      "grad_norm": 2.210606336593628,
      "learning_rate": 3.141964138838625e-06,
      "loss": 0.0341,
      "step": 10250
    },
    {
      "epoch": 54.21052631578947,
      "grad_norm": 0.02481137402355671,
      "learning_rate": 3.0597137687119592e-06,
      "loss": 0.0393,
      "step": 10300
    },
    {
      "epoch": 54.473684210526315,
      "grad_norm": 0.015779225155711174,
      "learning_rate": 2.977463398585294e-06,
      "loss": 0.038,
      "step": 10350
    },
    {
      "epoch": 54.73684210526316,
      "grad_norm": 0.018576575443148613,
      "learning_rate": 2.895213028458628e-06,
      "loss": 0.0233,
      "step": 10400
    },
    {
      "epoch": 55.0,
      "grad_norm": 0.015869276598095894,
      "learning_rate": 2.8129626583319627e-06,
      "loss": 0.0286,
      "step": 10450
    },
    {
      "epoch": 55.26315789473684,
      "grad_norm": 1.4139784574508667,
      "learning_rate": 2.730712288205297e-06,
      "loss": 0.0346,
      "step": 10500
    },
    {
      "epoch": 55.526315789473685,
      "grad_norm": 0.022119315341114998,
      "learning_rate": 2.648461918078631e-06,
      "loss": 0.0338,
      "step": 10550
    },
    {
      "epoch": 55.78947368421053,
      "grad_norm": 0.03452806919813156,
      "learning_rate": 2.566211547951966e-06,
      "loss": 0.0567,
      "step": 10600
    },
    {
      "epoch": 56.05263157894737,
      "grad_norm": 0.010135608725249767,
      "learning_rate": 2.4839611778253e-06,
      "loss": 0.0436,
      "step": 10650
    },
    {
      "epoch": 56.31578947368421,
      "grad_norm": 0.007302026264369488,
      "learning_rate": 2.4017108076986346e-06,
      "loss": 0.0331,
      "step": 10700
    },
    {
      "epoch": 56.578947368421055,
      "grad_norm": 0.019016169011592865,
      "learning_rate": 2.319460437571969e-06,
      "loss": 0.0303,
      "step": 10750
    },
    {
      "epoch": 56.8421052631579,
      "grad_norm": 0.026270441710948944,
      "learning_rate": 2.2372100674453036e-06,
      "loss": 0.0297,
      "step": 10800
    },
    {
      "epoch": 57.10526315789474,
      "grad_norm": 0.017941758036613464,
      "learning_rate": 2.154959697318638e-06,
      "loss": 0.0392,
      "step": 10850
    },
    {
      "epoch": 57.36842105263158,
      "grad_norm": 0.15702883899211884,
      "learning_rate": 2.0727093271919725e-06,
      "loss": 0.034,
      "step": 10900
    },
    {
      "epoch": 57.63157894736842,
      "grad_norm": 0.021275365725159645,
      "learning_rate": 1.990458957065307e-06,
      "loss": 0.0348,
      "step": 10950
    },
    {
      "epoch": 57.89473684210526,
      "grad_norm": 0.7689422965049744,
      "learning_rate": 1.9082085869386415e-06,
      "loss": 0.0363,
      "step": 11000
    },
    {
      "epoch": 58.1578947368421,
      "grad_norm": 10.515073776245117,
      "learning_rate": 1.8259582168119757e-06,
      "loss": 0.0513,
      "step": 11050
    },
    {
      "epoch": 58.421052631578945,
      "grad_norm": 10.06886100769043,
      "learning_rate": 1.7437078466853102e-06,
      "loss": 0.0537,
      "step": 11100
    },
    {
      "epoch": 58.68421052631579,
      "grad_norm": 7.557404518127441,
      "learning_rate": 1.6614574765586447e-06,
      "loss": 0.0217,
      "step": 11150
    },
    {
      "epoch": 58.94736842105263,
      "grad_norm": 12.977633476257324,
      "learning_rate": 1.5792071064319792e-06,
      "loss": 0.0307,
      "step": 11200
    },
    {
      "epoch": 59.21052631578947,
      "grad_norm": 0.010286446660757065,
      "learning_rate": 1.4969567363053137e-06,
      "loss": 0.0273,
      "step": 11250
    },
    {
      "epoch": 59.473684210526315,
      "grad_norm": 0.014062464237213135,
      "learning_rate": 1.4147063661786477e-06,
      "loss": 0.0495,
      "step": 11300
    },
    {
      "epoch": 59.73684210526316,
      "grad_norm": 1.4443079233169556,
      "learning_rate": 1.3324559960519822e-06,
      "loss": 0.0319,
      "step": 11350
    },
    {
      "epoch": 60.0,
      "grad_norm": 1.6750446557998657,
      "learning_rate": 1.2502056259253167e-06,
      "loss": 0.0274,
      "step": 11400
    },
    {
      "epoch": 60.26315789473684,
      "grad_norm": 0.011776186525821686,
      "learning_rate": 1.1679552557986511e-06,
      "loss": 0.0523,
      "step": 11450
    },
    {
      "epoch": 60.526315789473685,
      "grad_norm": 0.017941143363714218,
      "learning_rate": 1.0857048856719856e-06,
      "loss": 0.0395,
      "step": 11500
    },
    {
      "epoch": 60.78947368421053,
      "grad_norm": 8.423238754272461,
      "learning_rate": 1.00345451554532e-06,
      "loss": 0.0462,
      "step": 11550
    },
    {
      "epoch": 61.05263157894737,
      "grad_norm": 0.007660582195967436,
      "learning_rate": 9.212041454186545e-07,
      "loss": 0.0308,
      "step": 11600
    },
    {
      "epoch": 61.31578947368421,
      "grad_norm": 0.014639164321124554,
      "learning_rate": 8.389537752919889e-07,
      "loss": 0.0248,
      "step": 11650
    },
    {
      "epoch": 61.578947368421055,
      "grad_norm": 0.014649915508925915,
      "learning_rate": 7.567034051653232e-07,
      "loss": 0.0388,
      "step": 11700
    },
    {
      "epoch": 61.8421052631579,
      "grad_norm": 0.015580138191580772,
      "learning_rate": 6.744530350386577e-07,
      "loss": 0.0558,
      "step": 11750
    },
    {
      "epoch": 62.10526315789474,
      "grad_norm": 0.012222079560160637,
      "learning_rate": 5.922026649119922e-07,
      "loss": 0.0375,
      "step": 11800
    },
    {
      "epoch": 62.36842105263158,
      "grad_norm": 0.012783330865204334,
      "learning_rate": 5.099522947853265e-07,
      "loss": 0.0278,
      "step": 11850
    },
    {
      "epoch": 62.63157894736842,
      "grad_norm": 0.020032690837979317,
      "learning_rate": 4.2770192465866096e-07,
      "loss": 0.0364,
      "step": 11900
    },
    {
      "epoch": 62.89473684210526,
      "grad_norm": 0.013494456186890602,
      "learning_rate": 3.4545155453199544e-07,
      "loss": 0.0331,
      "step": 11950
    },
    {
      "epoch": 63.1578947368421,
      "grad_norm": 0.009963463060557842,
      "learning_rate": 2.6320118440532986e-07,
      "loss": 0.0611,
      "step": 12000
    },
    {
      "epoch": 63.421052631578945,
      "grad_norm": 0.21792468428611755,
      "learning_rate": 1.8095081427866426e-07,
      "loss": 0.0262,
      "step": 12050
    },
    {
      "epoch": 63.68421052631579,
      "grad_norm": 5.812948703765869,
      "learning_rate": 9.87004441519987e-08,
      "loss": 0.0377,
      "step": 12100
    },
    {
      "epoch": 63.94736842105263,
      "grad_norm": 0.009815866127610207,
      "learning_rate": 1.6450074025333116e-08,
      "loss": 0.0237,
      "step": 12150
    }
  ],
  "logging_steps": 50,
  "max_steps": 12160,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 64,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.190484727026483e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
