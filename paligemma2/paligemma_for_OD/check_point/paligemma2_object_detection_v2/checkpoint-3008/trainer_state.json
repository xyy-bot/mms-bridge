{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 62.67368421052632,
  "eval_steps": 500,
  "global_step": 3008,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.0421052631578946,
      "grad_norm": 5.291716575622559,
      "learning_rate": 1.9680638722554893e-05,
      "loss": 11.0228,
      "step": 50
    },
    {
      "epoch": 2.0842105263157893,
      "grad_norm": 8.152026176452637,
      "learning_rate": 1.9347970725216236e-05,
      "loss": 6.7324,
      "step": 100
    },
    {
      "epoch": 3.126315789473684,
      "grad_norm": 9.009628295898438,
      "learning_rate": 1.901530272787758e-05,
      "loss": 6.2819,
      "step": 150
    },
    {
      "epoch": 4.168421052631579,
      "grad_norm": 17.01360511779785,
      "learning_rate": 1.8682634730538923e-05,
      "loss": 5.8136,
      "step": 200
    },
    {
      "epoch": 5.2105263157894735,
      "grad_norm": 17.57796859741211,
      "learning_rate": 1.834996673320027e-05,
      "loss": 5.3714,
      "step": 250
    },
    {
      "epoch": 6.252631578947368,
      "grad_norm": 23.293264389038086,
      "learning_rate": 1.8017298735861613e-05,
      "loss": 4.9183,
      "step": 300
    },
    {
      "epoch": 7.294736842105263,
      "grad_norm": 29.69297218322754,
      "learning_rate": 1.7684630738522957e-05,
      "loss": 4.3821,
      "step": 350
    },
    {
      "epoch": 8.336842105263157,
      "grad_norm": 18.907419204711914,
      "learning_rate": 1.73519627411843e-05,
      "loss": 4.1394,
      "step": 400
    },
    {
      "epoch": 9.378947368421052,
      "grad_norm": 26.349021911621094,
      "learning_rate": 1.7019294743845644e-05,
      "loss": 3.6087,
      "step": 450
    },
    {
      "epoch": 10.421052631578947,
      "grad_norm": 30.705852508544922,
      "learning_rate": 1.6686626746506987e-05,
      "loss": 3.1417,
      "step": 500
    },
    {
      "epoch": 11.463157894736842,
      "grad_norm": 37.60873794555664,
      "learning_rate": 1.635395874916833e-05,
      "loss": 2.8311,
      "step": 550
    },
    {
      "epoch": 12.505263157894737,
      "grad_norm": 39.002498626708984,
      "learning_rate": 1.6021290751829674e-05,
      "loss": 2.2445,
      "step": 600
    },
    {
      "epoch": 13.547368421052632,
      "grad_norm": 28.577680587768555,
      "learning_rate": 1.5688622754491018e-05,
      "loss": 2.0686,
      "step": 650
    },
    {
      "epoch": 14.589473684210526,
      "grad_norm": 34.1461296081543,
      "learning_rate": 1.535595475715236e-05,
      "loss": 1.7365,
      "step": 700
    },
    {
      "epoch": 15.631578947368421,
      "grad_norm": 20.8606014251709,
      "learning_rate": 1.5023286759813706e-05,
      "loss": 1.4317,
      "step": 750
    },
    {
      "epoch": 16.673684210526314,
      "grad_norm": 19.774133682250977,
      "learning_rate": 1.4690618762475051e-05,
      "loss": 1.1835,
      "step": 800
    },
    {
      "epoch": 17.71578947368421,
      "grad_norm": 33.7147216796875,
      "learning_rate": 1.4357950765136395e-05,
      "loss": 0.946,
      "step": 850
    },
    {
      "epoch": 18.757894736842104,
      "grad_norm": 21.684833526611328,
      "learning_rate": 1.4025282767797738e-05,
      "loss": 0.7299,
      "step": 900
    },
    {
      "epoch": 19.8,
      "grad_norm": 19.176616668701172,
      "learning_rate": 1.3692614770459083e-05,
      "loss": 0.6044,
      "step": 950
    },
    {
      "epoch": 20.842105263157894,
      "grad_norm": 24.067602157592773,
      "learning_rate": 1.3359946773120427e-05,
      "loss": 0.5096,
      "step": 1000
    },
    {
      "epoch": 21.88421052631579,
      "grad_norm": 48.954898834228516,
      "learning_rate": 1.302727877578177e-05,
      "loss": 0.4302,
      "step": 1050
    },
    {
      "epoch": 22.926315789473684,
      "grad_norm": 38.66605758666992,
      "learning_rate": 1.2694610778443115e-05,
      "loss": 0.4036,
      "step": 1100
    },
    {
      "epoch": 23.96842105263158,
      "grad_norm": 11.68008041381836,
      "learning_rate": 1.2361942781104459e-05,
      "loss": 0.347,
      "step": 1150
    },
    {
      "epoch": 25.0,
      "grad_norm": 5.601789951324463,
      "learning_rate": 1.2029274783765802e-05,
      "loss": 0.3211,
      "step": 1200
    },
    {
      "epoch": 26.042105263157893,
      "grad_norm": 20.935222625732422,
      "learning_rate": 1.1696606786427148e-05,
      "loss": 0.2962,
      "step": 1250
    },
    {
      "epoch": 27.08421052631579,
      "grad_norm": 32.846675872802734,
      "learning_rate": 1.1363938789088491e-05,
      "loss": 0.2599,
      "step": 1300
    },
    {
      "epoch": 28.126315789473683,
      "grad_norm": 26.203153610229492,
      "learning_rate": 1.1031270791749834e-05,
      "loss": 0.2462,
      "step": 1350
    },
    {
      "epoch": 29.16842105263158,
      "grad_norm": 3.6861422061920166,
      "learning_rate": 1.069860279441118e-05,
      "loss": 0.2342,
      "step": 1400
    },
    {
      "epoch": 30.210526315789473,
      "grad_norm": 19.636857986450195,
      "learning_rate": 1.0365934797072523e-05,
      "loss": 0.2302,
      "step": 1450
    },
    {
      "epoch": 31.25263157894737,
      "grad_norm": 24.290578842163086,
      "learning_rate": 1.0033266799733866e-05,
      "loss": 0.2161,
      "step": 1500
    },
    {
      "epoch": 32.294736842105266,
      "grad_norm": 27.5845947265625,
      "learning_rate": 9.70059880239521e-06,
      "loss": 0.2086,
      "step": 1550
    },
    {
      "epoch": 33.33684210526316,
      "grad_norm": 29.426082611083984,
      "learning_rate": 9.367930805056555e-06,
      "loss": 0.2078,
      "step": 1600
    },
    {
      "epoch": 34.37894736842105,
      "grad_norm": 16.38437843322754,
      "learning_rate": 9.035262807717899e-06,
      "loss": 0.1658,
      "step": 1650
    },
    {
      "epoch": 35.421052631578945,
      "grad_norm": 24.341106414794922,
      "learning_rate": 8.702594810379242e-06,
      "loss": 0.1965,
      "step": 1700
    },
    {
      "epoch": 36.463157894736845,
      "grad_norm": 15.486903190612793,
      "learning_rate": 8.369926813040587e-06,
      "loss": 0.1692,
      "step": 1750
    },
    {
      "epoch": 37.50526315789474,
      "grad_norm": 16.24987030029297,
      "learning_rate": 8.03725881570193e-06,
      "loss": 0.1922,
      "step": 1800
    },
    {
      "epoch": 38.54736842105263,
      "grad_norm": 10.94313907623291,
      "learning_rate": 7.704590818363274e-06,
      "loss": 0.1226,
      "step": 1850
    },
    {
      "epoch": 39.589473684210525,
      "grad_norm": 0.7221121788024902,
      "learning_rate": 7.371922821024618e-06,
      "loss": 0.1792,
      "step": 1900
    },
    {
      "epoch": 40.63157894736842,
      "grad_norm": 0.5822381973266602,
      "learning_rate": 7.039254823685962e-06,
      "loss": 0.1839,
      "step": 1950
    },
    {
      "epoch": 41.67368421052632,
      "grad_norm": 0.6498748064041138,
      "learning_rate": 6.706586826347305e-06,
      "loss": 0.183,
      "step": 2000
    },
    {
      "epoch": 42.71578947368421,
      "grad_norm": 12.98048210144043,
      "learning_rate": 6.3739188290086496e-06,
      "loss": 0.1655,
      "step": 2050
    },
    {
      "epoch": 43.757894736842104,
      "grad_norm": 10.620429992675781,
      "learning_rate": 6.041250831669994e-06,
      "loss": 0.158,
      "step": 2100
    },
    {
      "epoch": 44.8,
      "grad_norm": 9.997699737548828,
      "learning_rate": 5.708582834331337e-06,
      "loss": 0.1586,
      "step": 2150
    },
    {
      "epoch": 45.8421052631579,
      "grad_norm": 18.452728271484375,
      "learning_rate": 5.375914836992682e-06,
      "loss": 0.13,
      "step": 2200
    },
    {
      "epoch": 46.88421052631579,
      "grad_norm": 0.36535927653312683,
      "learning_rate": 5.043246839654026e-06,
      "loss": 0.1593,
      "step": 2250
    },
    {
      "epoch": 47.92631578947368,
      "grad_norm": 29.151342391967773,
      "learning_rate": 4.710578842315369e-06,
      "loss": 0.163,
      "step": 2300
    },
    {
      "epoch": 48.96842105263158,
      "grad_norm": 15.709929466247559,
      "learning_rate": 4.377910844976714e-06,
      "loss": 0.1466,
      "step": 2350
    },
    {
      "epoch": 50.0,
      "grad_norm": 0.415569543838501,
      "learning_rate": 4.045242847638057e-06,
      "loss": 0.1247,
      "step": 2400
    },
    {
      "epoch": 51.04210526315789,
      "grad_norm": 13.748950004577637,
      "learning_rate": 3.7125748502994014e-06,
      "loss": 0.1313,
      "step": 2450
    },
    {
      "epoch": 52.084210526315786,
      "grad_norm": 9.614873886108398,
      "learning_rate": 3.3799068529607453e-06,
      "loss": 0.1532,
      "step": 2500
    },
    {
      "epoch": 53.126315789473686,
      "grad_norm": 18.422134399414062,
      "learning_rate": 3.0472388556220896e-06,
      "loss": 0.1187,
      "step": 2550
    },
    {
      "epoch": 54.16842105263158,
      "grad_norm": 19.76861000061035,
      "learning_rate": 2.7145708582834335e-06,
      "loss": 0.1312,
      "step": 2600
    },
    {
      "epoch": 55.21052631578947,
      "grad_norm": 22.82573890686035,
      "learning_rate": 2.3819028609447774e-06,
      "loss": 0.1061,
      "step": 2650
    },
    {
      "epoch": 56.252631578947366,
      "grad_norm": 2.1270205974578857,
      "learning_rate": 2.0492348636061213e-06,
      "loss": 0.1428,
      "step": 2700
    },
    {
      "epoch": 57.294736842105266,
      "grad_norm": 7.668252944946289,
      "learning_rate": 1.7165668662674654e-06,
      "loss": 0.1249,
      "step": 2750
    },
    {
      "epoch": 58.33684210526316,
      "grad_norm": 21.409826278686523,
      "learning_rate": 1.383898868928809e-06,
      "loss": 0.1377,
      "step": 2800
    },
    {
      "epoch": 59.37894736842105,
      "grad_norm": 30.248167037963867,
      "learning_rate": 1.0512308715901531e-06,
      "loss": 0.1268,
      "step": 2850
    },
    {
      "epoch": 60.421052631578945,
      "grad_norm": 5.441479206085205,
      "learning_rate": 7.18562874251497e-07,
      "loss": 0.1265,
      "step": 2900
    },
    {
      "epoch": 61.463157894736845,
      "grad_norm": 0.1867339015007019,
      "learning_rate": 3.8589487691284103e-07,
      "loss": 0.1068,
      "step": 2950
    },
    {
      "epoch": 62.50526315789474,
      "grad_norm": 23.084545135498047,
      "learning_rate": 5.322687957418497e-08,
      "loss": 0.1361,
      "step": 3000
    }
  ],
  "logging_steps": 50,
  "max_steps": 3008,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 64,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.082886931190451e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
