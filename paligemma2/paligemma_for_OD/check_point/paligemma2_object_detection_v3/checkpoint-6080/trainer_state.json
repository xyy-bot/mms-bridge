{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 64.0,
  "eval_steps": 500,
  "global_step": 6080,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 6.556708335876465,
      "learning_rate": 1.9842053307008887e-05,
      "loss": 5.6633,
      "step": 50
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 5.288947582244873,
      "learning_rate": 1.9677525501809807e-05,
      "loss": 3.598,
      "step": 100
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 10.125821113586426,
      "learning_rate": 1.951299769661073e-05,
      "loss": 3.3077,
      "step": 150
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 8.517317771911621,
      "learning_rate": 1.934846989141165e-05,
      "loss": 3.1861,
      "step": 200
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 17.16397476196289,
      "learning_rate": 1.918394208621257e-05,
      "loss": 3.0173,
      "step": 250
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 9.440197944641113,
      "learning_rate": 1.901941428101349e-05,
      "loss": 2.905,
      "step": 300
    },
    {
      "epoch": 3.6842105263157894,
      "grad_norm": 18.428159713745117,
      "learning_rate": 1.8854886475814412e-05,
      "loss": 2.7559,
      "step": 350
    },
    {
      "epoch": 4.2105263157894735,
      "grad_norm": 15.903705596923828,
      "learning_rate": 1.8690358670615336e-05,
      "loss": 2.5623,
      "step": 400
    },
    {
      "epoch": 4.7368421052631575,
      "grad_norm": 17.971012115478516,
      "learning_rate": 1.8525830865416256e-05,
      "loss": 2.4042,
      "step": 450
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 15.063633918762207,
      "learning_rate": 1.8361303060217176e-05,
      "loss": 2.3245,
      "step": 500
    },
    {
      "epoch": 5.7894736842105265,
      "grad_norm": 29.217166900634766,
      "learning_rate": 1.81967752550181e-05,
      "loss": 2.1771,
      "step": 550
    },
    {
      "epoch": 6.315789473684211,
      "grad_norm": 13.432668685913086,
      "learning_rate": 1.803224744981902e-05,
      "loss": 1.9309,
      "step": 600
    },
    {
      "epoch": 6.842105263157895,
      "grad_norm": 27.177749633789062,
      "learning_rate": 1.7867719644619944e-05,
      "loss": 1.8747,
      "step": 650
    },
    {
      "epoch": 7.368421052631579,
      "grad_norm": 16.561481475830078,
      "learning_rate": 1.7703191839420864e-05,
      "loss": 1.7492,
      "step": 700
    },
    {
      "epoch": 7.894736842105263,
      "grad_norm": 20.00844955444336,
      "learning_rate": 1.7538664034221785e-05,
      "loss": 1.7211,
      "step": 750
    },
    {
      "epoch": 8.421052631578947,
      "grad_norm": 17.709457397460938,
      "learning_rate": 1.737413622902271e-05,
      "loss": 1.5047,
      "step": 800
    },
    {
      "epoch": 8.947368421052632,
      "grad_norm": 33.32740020751953,
      "learning_rate": 1.720960842382363e-05,
      "loss": 1.5117,
      "step": 850
    },
    {
      "epoch": 9.473684210526315,
      "grad_norm": 11.596896171569824,
      "learning_rate": 1.704508061862455e-05,
      "loss": 1.2651,
      "step": 900
    },
    {
      "epoch": 10.0,
      "grad_norm": 19.240182876586914,
      "learning_rate": 1.688055281342547e-05,
      "loss": 1.3034,
      "step": 950
    },
    {
      "epoch": 10.526315789473685,
      "grad_norm": 21.834463119506836,
      "learning_rate": 1.671602500822639e-05,
      "loss": 1.0692,
      "step": 1000
    },
    {
      "epoch": 11.052631578947368,
      "grad_norm": 10.909173965454102,
      "learning_rate": 1.6551497203027313e-05,
      "loss": 1.0765,
      "step": 1050
    },
    {
      "epoch": 11.578947368421053,
      "grad_norm": 30.16254234313965,
      "learning_rate": 1.6386969397828234e-05,
      "loss": 0.8876,
      "step": 1100
    },
    {
      "epoch": 12.105263157894736,
      "grad_norm": 26.786354064941406,
      "learning_rate": 1.6222441592629157e-05,
      "loss": 0.8832,
      "step": 1150
    },
    {
      "epoch": 12.631578947368421,
      "grad_norm": 25.496580123901367,
      "learning_rate": 1.6057913787430078e-05,
      "loss": 0.7276,
      "step": 1200
    },
    {
      "epoch": 13.157894736842104,
      "grad_norm": 27.25625991821289,
      "learning_rate": 1.5893385982230998e-05,
      "loss": 0.8363,
      "step": 1250
    },
    {
      "epoch": 13.68421052631579,
      "grad_norm": 30.38800811767578,
      "learning_rate": 1.5728858177031922e-05,
      "loss": 0.5838,
      "step": 1300
    },
    {
      "epoch": 14.210526315789474,
      "grad_norm": 31.68623161315918,
      "learning_rate": 1.5564330371832842e-05,
      "loss": 0.6634,
      "step": 1350
    },
    {
      "epoch": 14.736842105263158,
      "grad_norm": 33.116111755371094,
      "learning_rate": 1.5399802566633762e-05,
      "loss": 0.517,
      "step": 1400
    },
    {
      "epoch": 15.263157894736842,
      "grad_norm": 31.331769943237305,
      "learning_rate": 1.5235274761434684e-05,
      "loss": 0.48,
      "step": 1450
    },
    {
      "epoch": 15.789473684210526,
      "grad_norm": 7.381961822509766,
      "learning_rate": 1.5070746956235605e-05,
      "loss": 0.4528,
      "step": 1500
    },
    {
      "epoch": 16.31578947368421,
      "grad_norm": 10.40454387664795,
      "learning_rate": 1.4906219151036527e-05,
      "loss": 0.4097,
      "step": 1550
    },
    {
      "epoch": 16.842105263157894,
      "grad_norm": 21.80838966369629,
      "learning_rate": 1.4741691345837447e-05,
      "loss": 0.3607,
      "step": 1600
    },
    {
      "epoch": 17.36842105263158,
      "grad_norm": 10.169039726257324,
      "learning_rate": 1.4577163540638367e-05,
      "loss": 0.3322,
      "step": 1650
    },
    {
      "epoch": 17.894736842105264,
      "grad_norm": 11.186600685119629,
      "learning_rate": 1.4412635735439291e-05,
      "loss": 0.344,
      "step": 1700
    },
    {
      "epoch": 18.42105263157895,
      "grad_norm": 9.200201988220215,
      "learning_rate": 1.4248107930240212e-05,
      "loss": 0.2302,
      "step": 1750
    },
    {
      "epoch": 18.94736842105263,
      "grad_norm": 24.98440933227539,
      "learning_rate": 1.4083580125041134e-05,
      "loss": 0.2856,
      "step": 1800
    },
    {
      "epoch": 19.473684210526315,
      "grad_norm": 10.875836372375488,
      "learning_rate": 1.3919052319842054e-05,
      "loss": 0.1973,
      "step": 1850
    },
    {
      "epoch": 20.0,
      "grad_norm": 1.9610416889190674,
      "learning_rate": 1.3754524514642976e-05,
      "loss": 0.2145,
      "step": 1900
    },
    {
      "epoch": 20.526315789473685,
      "grad_norm": 13.75447940826416,
      "learning_rate": 1.3589996709443898e-05,
      "loss": 0.2079,
      "step": 1950
    },
    {
      "epoch": 21.05263157894737,
      "grad_norm": 19.99791145324707,
      "learning_rate": 1.3425468904244818e-05,
      "loss": 0.2184,
      "step": 2000
    },
    {
      "epoch": 21.57894736842105,
      "grad_norm": 7.4890265464782715,
      "learning_rate": 1.3260941099045739e-05,
      "loss": 0.1275,
      "step": 2050
    },
    {
      "epoch": 22.105263157894736,
      "grad_norm": 41.4512825012207,
      "learning_rate": 1.3096413293846662e-05,
      "loss": 0.2323,
      "step": 2100
    },
    {
      "epoch": 22.63157894736842,
      "grad_norm": 18.634788513183594,
      "learning_rate": 1.2931885488647583e-05,
      "loss": 0.1391,
      "step": 2150
    },
    {
      "epoch": 23.157894736842106,
      "grad_norm": 2.903923749923706,
      "learning_rate": 1.2767357683448505e-05,
      "loss": 0.139,
      "step": 2200
    },
    {
      "epoch": 23.68421052631579,
      "grad_norm": 23.450204849243164,
      "learning_rate": 1.2602829878249425e-05,
      "loss": 0.1578,
      "step": 2250
    },
    {
      "epoch": 24.210526315789473,
      "grad_norm": 0.3670978546142578,
      "learning_rate": 1.2438302073050345e-05,
      "loss": 0.132,
      "step": 2300
    },
    {
      "epoch": 24.736842105263158,
      "grad_norm": 3.3413684368133545,
      "learning_rate": 1.2273774267851269e-05,
      "loss": 0.1383,
      "step": 2350
    },
    {
      "epoch": 25.263157894736842,
      "grad_norm": 33.111148834228516,
      "learning_rate": 1.210924646265219e-05,
      "loss": 0.1773,
      "step": 2400
    },
    {
      "epoch": 25.789473684210527,
      "grad_norm": 21.56681251525879,
      "learning_rate": 1.1944718657453111e-05,
      "loss": 0.1453,
      "step": 2450
    },
    {
      "epoch": 26.31578947368421,
      "grad_norm": 17.460739135742188,
      "learning_rate": 1.1780190852254032e-05,
      "loss": 0.1252,
      "step": 2500
    },
    {
      "epoch": 26.842105263157894,
      "grad_norm": 28.25884437561035,
      "learning_rate": 1.1615663047054952e-05,
      "loss": 0.0951,
      "step": 2550
    },
    {
      "epoch": 27.36842105263158,
      "grad_norm": 21.947357177734375,
      "learning_rate": 1.1451135241855876e-05,
      "loss": 0.1344,
      "step": 2600
    },
    {
      "epoch": 27.894736842105264,
      "grad_norm": 13.602782249450684,
      "learning_rate": 1.1286607436656796e-05,
      "loss": 0.1216,
      "step": 2650
    },
    {
      "epoch": 28.42105263157895,
      "grad_norm": 12.406327247619629,
      "learning_rate": 1.1122079631457716e-05,
      "loss": 0.1403,
      "step": 2700
    },
    {
      "epoch": 28.94736842105263,
      "grad_norm": 0.47141197323799133,
      "learning_rate": 1.0957551826258638e-05,
      "loss": 0.1188,
      "step": 2750
    },
    {
      "epoch": 29.473684210526315,
      "grad_norm": 2.1888322830200195,
      "learning_rate": 1.0793024021059559e-05,
      "loss": 0.1158,
      "step": 2800
    },
    {
      "epoch": 30.0,
      "grad_norm": 50.43977355957031,
      "learning_rate": 1.0628496215860482e-05,
      "loss": 0.1096,
      "step": 2850
    },
    {
      "epoch": 30.526315789473685,
      "grad_norm": 21.44258689880371,
      "learning_rate": 1.0463968410661403e-05,
      "loss": 0.0847,
      "step": 2900
    },
    {
      "epoch": 31.05263157894737,
      "grad_norm": 0.6502317190170288,
      "learning_rate": 1.0299440605462323e-05,
      "loss": 0.1359,
      "step": 2950
    },
    {
      "epoch": 31.57894736842105,
      "grad_norm": 30.654464721679688,
      "learning_rate": 1.0134912800263245e-05,
      "loss": 0.167,
      "step": 3000
    },
    {
      "epoch": 32.10526315789474,
      "grad_norm": 0.19179052114486694,
      "learning_rate": 9.970384995064165e-06,
      "loss": 0.0884,
      "step": 3050
    },
    {
      "epoch": 32.63157894736842,
      "grad_norm": 12.162328720092773,
      "learning_rate": 9.805857189865087e-06,
      "loss": 0.1003,
      "step": 3100
    },
    {
      "epoch": 33.1578947368421,
      "grad_norm": 16.145078659057617,
      "learning_rate": 9.64132938466601e-06,
      "loss": 0.127,
      "step": 3150
    },
    {
      "epoch": 33.68421052631579,
      "grad_norm": 6.138287544250488,
      "learning_rate": 9.476801579466932e-06,
      "loss": 0.1024,
      "step": 3200
    },
    {
      "epoch": 34.21052631578947,
      "grad_norm": 0.15633860230445862,
      "learning_rate": 9.312273774267852e-06,
      "loss": 0.1126,
      "step": 3250
    },
    {
      "epoch": 34.73684210526316,
      "grad_norm": 8.835330963134766,
      "learning_rate": 9.147745969068774e-06,
      "loss": 0.1177,
      "step": 3300
    },
    {
      "epoch": 35.26315789473684,
      "grad_norm": 0.5289728045463562,
      "learning_rate": 8.983218163869694e-06,
      "loss": 0.0984,
      "step": 3350
    },
    {
      "epoch": 35.78947368421053,
      "grad_norm": 0.06309042125940323,
      "learning_rate": 8.818690358670616e-06,
      "loss": 0.0867,
      "step": 3400
    },
    {
      "epoch": 36.31578947368421,
      "grad_norm": 0.13848581910133362,
      "learning_rate": 8.654162553471538e-06,
      "loss": 0.0532,
      "step": 3450
    },
    {
      "epoch": 36.8421052631579,
      "grad_norm": 0.10042577981948853,
      "learning_rate": 8.489634748272459e-06,
      "loss": 0.0999,
      "step": 3500
    },
    {
      "epoch": 37.36842105263158,
      "grad_norm": 17.576431274414062,
      "learning_rate": 8.32510694307338e-06,
      "loss": 0.1191,
      "step": 3550
    },
    {
      "epoch": 37.89473684210526,
      "grad_norm": 0.069442979991436,
      "learning_rate": 8.160579137874301e-06,
      "loss": 0.0735,
      "step": 3600
    },
    {
      "epoch": 38.421052631578945,
      "grad_norm": 0.1479109227657318,
      "learning_rate": 7.996051332675223e-06,
      "loss": 0.0874,
      "step": 3650
    },
    {
      "epoch": 38.94736842105263,
      "grad_norm": 0.18698090314865112,
      "learning_rate": 7.831523527476143e-06,
      "loss": 0.065,
      "step": 3700
    },
    {
      "epoch": 39.473684210526315,
      "grad_norm": 2.0116984844207764,
      "learning_rate": 7.666995722277065e-06,
      "loss": 0.1317,
      "step": 3750
    },
    {
      "epoch": 40.0,
      "grad_norm": 17.816898345947266,
      "learning_rate": 7.5024679170779865e-06,
      "loss": 0.1079,
      "step": 3800
    },
    {
      "epoch": 40.526315789473685,
      "grad_norm": 5.533644199371338,
      "learning_rate": 7.3379401118789085e-06,
      "loss": 0.0837,
      "step": 3850
    },
    {
      "epoch": 41.05263157894737,
      "grad_norm": 0.16253222525119781,
      "learning_rate": 7.17341230667983e-06,
      "loss": 0.1056,
      "step": 3900
    },
    {
      "epoch": 41.578947368421055,
      "grad_norm": 0.26222434639930725,
      "learning_rate": 7.008884501480751e-06,
      "loss": 0.077,
      "step": 3950
    },
    {
      "epoch": 42.10526315789474,
      "grad_norm": 3.9126737117767334,
      "learning_rate": 6.844356696281672e-06,
      "loss": 0.1053,
      "step": 4000
    },
    {
      "epoch": 42.63157894736842,
      "grad_norm": 34.44890213012695,
      "learning_rate": 6.679828891082594e-06,
      "loss": 0.0978,
      "step": 4050
    },
    {
      "epoch": 43.1578947368421,
      "grad_norm": 0.6630411148071289,
      "learning_rate": 6.515301085883515e-06,
      "loss": 0.0969,
      "step": 4100
    },
    {
      "epoch": 43.68421052631579,
      "grad_norm": 32.149383544921875,
      "learning_rate": 6.3507732806844355e-06,
      "loss": 0.0976,
      "step": 4150
    },
    {
      "epoch": 44.21052631578947,
      "grad_norm": 0.03773599863052368,
      "learning_rate": 6.1862454754853576e-06,
      "loss": 0.1087,
      "step": 4200
    },
    {
      "epoch": 44.73684210526316,
      "grad_norm": 18.523822784423828,
      "learning_rate": 6.021717670286279e-06,
      "loss": 0.0843,
      "step": 4250
    },
    {
      "epoch": 45.26315789473684,
      "grad_norm": 13.146002769470215,
      "learning_rate": 5.857189865087201e-06,
      "loss": 0.0779,
      "step": 4300
    },
    {
      "epoch": 45.78947368421053,
      "grad_norm": 8.812846183776855,
      "learning_rate": 5.692662059888121e-06,
      "loss": 0.0533,
      "step": 4350
    },
    {
      "epoch": 46.31578947368421,
      "grad_norm": 22.02577018737793,
      "learning_rate": 5.528134254689042e-06,
      "loss": 0.1155,
      "step": 4400
    },
    {
      "epoch": 46.8421052631579,
      "grad_norm": 0.12798833847045898,
      "learning_rate": 5.363606449489964e-06,
      "loss": 0.0798,
      "step": 4450
    },
    {
      "epoch": 47.36842105263158,
      "grad_norm": 23.26314353942871,
      "learning_rate": 5.1990786442908855e-06,
      "loss": 0.0866,
      "step": 4500
    },
    {
      "epoch": 47.89473684210526,
      "grad_norm": 16.216232299804688,
      "learning_rate": 5.0345508390918075e-06,
      "loss": 0.0902,
      "step": 4550
    },
    {
      "epoch": 48.421052631578945,
      "grad_norm": 0.10452894121408463,
      "learning_rate": 4.870023033892729e-06,
      "loss": 0.0725,
      "step": 4600
    },
    {
      "epoch": 48.94736842105263,
      "grad_norm": 14.811261177062988,
      "learning_rate": 4.70549522869365e-06,
      "loss": 0.0764,
      "step": 4650
    },
    {
      "epoch": 49.473684210526315,
      "grad_norm": 0.21821168065071106,
      "learning_rate": 4.540967423494571e-06,
      "loss": 0.0797,
      "step": 4700
    },
    {
      "epoch": 50.0,
      "grad_norm": 0.07191196084022522,
      "learning_rate": 4.376439618295492e-06,
      "loss": 0.0694,
      "step": 4750
    },
    {
      "epoch": 50.526315789473685,
      "grad_norm": 27.744977951049805,
      "learning_rate": 4.211911813096413e-06,
      "loss": 0.0843,
      "step": 4800
    },
    {
      "epoch": 51.05263157894737,
      "grad_norm": 3.496446371078491,
      "learning_rate": 4.0473840078973345e-06,
      "loss": 0.0594,
      "step": 4850
    },
    {
      "epoch": 51.578947368421055,
      "grad_norm": 0.06907348334789276,
      "learning_rate": 3.8828562026982565e-06,
      "loss": 0.1037,
      "step": 4900
    },
    {
      "epoch": 52.10526315789474,
      "grad_norm": 0.04735434800386429,
      "learning_rate": 3.7183283974991773e-06,
      "loss": 0.0876,
      "step": 4950
    },
    {
      "epoch": 52.63157894736842,
      "grad_norm": 0.07225965708494186,
      "learning_rate": 3.553800592300099e-06,
      "loss": 0.0753,
      "step": 5000
    },
    {
      "epoch": 53.1578947368421,
      "grad_norm": 2.7736947536468506,
      "learning_rate": 3.3892727871010205e-06,
      "loss": 0.0584,
      "step": 5050
    },
    {
      "epoch": 53.68421052631579,
      "grad_norm": 9.65572738647461,
      "learning_rate": 3.2247449819019417e-06,
      "loss": 0.0947,
      "step": 5100
    },
    {
      "epoch": 54.21052631578947,
      "grad_norm": 11.793158531188965,
      "learning_rate": 3.0602171767028633e-06,
      "loss": 0.0603,
      "step": 5150
    },
    {
      "epoch": 54.73684210526316,
      "grad_norm": 0.03585636615753174,
      "learning_rate": 2.8956893715037844e-06,
      "loss": 0.0489,
      "step": 5200
    },
    {
      "epoch": 55.26315789473684,
      "grad_norm": 3.3857696056365967,
      "learning_rate": 2.731161566304706e-06,
      "loss": 0.0643,
      "step": 5250
    },
    {
      "epoch": 55.78947368421053,
      "grad_norm": 0.06774023175239563,
      "learning_rate": 2.566633761105627e-06,
      "loss": 0.0885,
      "step": 5300
    },
    {
      "epoch": 56.31578947368421,
      "grad_norm": 0.03999847546219826,
      "learning_rate": 2.4021059559065484e-06,
      "loss": 0.0753,
      "step": 5350
    },
    {
      "epoch": 56.8421052631579,
      "grad_norm": 0.027895307168364525,
      "learning_rate": 2.23757815070747e-06,
      "loss": 0.0614,
      "step": 5400
    },
    {
      "epoch": 57.36842105263158,
      "grad_norm": 0.3301844298839569,
      "learning_rate": 2.073050345508391e-06,
      "loss": 0.0749,
      "step": 5450
    },
    {
      "epoch": 57.89473684210526,
      "grad_norm": 13.299485206604004,
      "learning_rate": 1.9085225403093123e-06,
      "loss": 0.0698,
      "step": 5500
    },
    {
      "epoch": 58.421052631578945,
      "grad_norm": 13.791135787963867,
      "learning_rate": 1.7439947351102337e-06,
      "loss": 0.1012,
      "step": 5550
    },
    {
      "epoch": 58.94736842105263,
      "grad_norm": 16.458820343017578,
      "learning_rate": 1.5794669299111551e-06,
      "loss": 0.06,
      "step": 5600
    },
    {
      "epoch": 59.473684210526315,
      "grad_norm": 0.6230286955833435,
      "learning_rate": 1.4149391247120765e-06,
      "loss": 0.0796,
      "step": 5650
    },
    {
      "epoch": 60.0,
      "grad_norm": 8.857967376708984,
      "learning_rate": 1.2504113195129977e-06,
      "loss": 0.06,
      "step": 5700
    },
    {
      "epoch": 60.526315789473685,
      "grad_norm": 0.05947231501340866,
      "learning_rate": 1.0858835143139193e-06,
      "loss": 0.0828,
      "step": 5750
    },
    {
      "epoch": 61.05263157894737,
      "grad_norm": 0.024267608299851418,
      "learning_rate": 9.213557091148404e-07,
      "loss": 0.0733,
      "step": 5800
    },
    {
      "epoch": 61.578947368421055,
      "grad_norm": 0.05330391600728035,
      "learning_rate": 7.568279039157618e-07,
      "loss": 0.0554,
      "step": 5850
    },
    {
      "epoch": 62.10526315789474,
      "grad_norm": 0.03098248690366745,
      "learning_rate": 5.923000987166832e-07,
      "loss": 0.0895,
      "step": 5900
    },
    {
      "epoch": 62.63157894736842,
      "grad_norm": 12.222847938537598,
      "learning_rate": 4.277722935176045e-07,
      "loss": 0.0621,
      "step": 5950
    },
    {
      "epoch": 63.1578947368421,
      "grad_norm": 13.09544563293457,
      "learning_rate": 2.6324448831852583e-07,
      "loss": 0.0936,
      "step": 6000
    },
    {
      "epoch": 63.68421052631579,
      "grad_norm": 11.407964706420898,
      "learning_rate": 9.87166831194472e-08,
      "loss": 0.0671,
      "step": 6050
    }
  ],
  "logging_steps": 50,
  "max_steps": 6080,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 64,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.190484727026483e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
