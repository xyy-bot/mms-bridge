{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 39.504731861198735,
  "eval_steps": 500,
  "global_step": 3160,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.6309148264984227,
      "grad_norm": 9.861983299255371,
      "learning_rate": 1.969601013299557e-05,
      "loss": 11.7302,
      "step": 50
    },
    {
      "epoch": 1.252365930599369,
      "grad_norm": 20.44124412536621,
      "learning_rate": 1.9379354021532618e-05,
      "loss": 6.8036,
      "step": 100
    },
    {
      "epoch": 1.8832807570977916,
      "grad_norm": 18.368471145629883,
      "learning_rate": 1.9062697910069666e-05,
      "loss": 6.6066,
      "step": 150
    },
    {
      "epoch": 2.504731861198738,
      "grad_norm": 21.660146713256836,
      "learning_rate": 1.8746041798606717e-05,
      "loss": 6.1814,
      "step": 200
    },
    {
      "epoch": 3.1261829652996846,
      "grad_norm": 21.022268295288086,
      "learning_rate": 1.8429385687143765e-05,
      "loss": 5.9788,
      "step": 250
    },
    {
      "epoch": 3.757097791798107,
      "grad_norm": 19.316381454467773,
      "learning_rate": 1.8112729575680813e-05,
      "loss": 5.7774,
      "step": 300
    },
    {
      "epoch": 4.378548895899054,
      "grad_norm": 20.532859802246094,
      "learning_rate": 1.779607346421786e-05,
      "loss": 5.2549,
      "step": 350
    },
    {
      "epoch": 5.0,
      "grad_norm": 9.268683433532715,
      "learning_rate": 1.747941735275491e-05,
      "loss": 5.2181,
      "step": 400
    },
    {
      "epoch": 5.630914826498422,
      "grad_norm": 31.73149299621582,
      "learning_rate": 1.716276124129196e-05,
      "loss": 4.8554,
      "step": 450
    },
    {
      "epoch": 6.252365930599369,
      "grad_norm": 20.779691696166992,
      "learning_rate": 1.6846105129829007e-05,
      "loss": 4.6425,
      "step": 500
    },
    {
      "epoch": 6.883280757097792,
      "grad_norm": 23.33897590637207,
      "learning_rate": 1.6529449018366055e-05,
      "loss": 4.4109,
      "step": 550
    },
    {
      "epoch": 7.504731861198739,
      "grad_norm": 29.282854080200195,
      "learning_rate": 1.6212792906903106e-05,
      "loss": 3.9942,
      "step": 600
    },
    {
      "epoch": 8.126182965299684,
      "grad_norm": 33.05330276489258,
      "learning_rate": 1.5896136795440154e-05,
      "loss": 4.0014,
      "step": 650
    },
    {
      "epoch": 8.757097791798108,
      "grad_norm": 34.80656433105469,
      "learning_rate": 1.55794806839772e-05,
      "loss": 3.6836,
      "step": 700
    },
    {
      "epoch": 9.378548895899053,
      "grad_norm": 26.139389038085938,
      "learning_rate": 1.526282457251425e-05,
      "loss": 3.5155,
      "step": 750
    },
    {
      "epoch": 10.0,
      "grad_norm": 15.283304214477539,
      "learning_rate": 1.49461684610513e-05,
      "loss": 3.3697,
      "step": 800
    },
    {
      "epoch": 10.630914826498422,
      "grad_norm": 39.179779052734375,
      "learning_rate": 1.4629512349588348e-05,
      "loss": 3.0925,
      "step": 850
    },
    {
      "epoch": 11.25236593059937,
      "grad_norm": 41.96982955932617,
      "learning_rate": 1.4312856238125396e-05,
      "loss": 2.8652,
      "step": 900
    },
    {
      "epoch": 11.883280757097792,
      "grad_norm": 35.05557632446289,
      "learning_rate": 1.3996200126662446e-05,
      "loss": 2.771,
      "step": 950
    },
    {
      "epoch": 12.504731861198739,
      "grad_norm": 30.820642471313477,
      "learning_rate": 1.3679544015199495e-05,
      "loss": 2.4816,
      "step": 1000
    },
    {
      "epoch": 13.126182965299684,
      "grad_norm": 33.472900390625,
      "learning_rate": 1.3362887903736543e-05,
      "loss": 2.3345,
      "step": 1050
    },
    {
      "epoch": 13.757097791798108,
      "grad_norm": 57.47579574584961,
      "learning_rate": 1.3046231792273592e-05,
      "loss": 2.2187,
      "step": 1100
    },
    {
      "epoch": 14.378548895899053,
      "grad_norm": 47.09660720825195,
      "learning_rate": 1.272957568081064e-05,
      "loss": 1.9028,
      "step": 1150
    },
    {
      "epoch": 15.0,
      "grad_norm": 31.923412322998047,
      "learning_rate": 1.241291956934769e-05,
      "loss": 1.9004,
      "step": 1200
    },
    {
      "epoch": 15.630914826498422,
      "grad_norm": 57.70624923706055,
      "learning_rate": 1.2096263457884737e-05,
      "loss": 1.532,
      "step": 1250
    },
    {
      "epoch": 16.252365930599368,
      "grad_norm": 34.60414505004883,
      "learning_rate": 1.1779607346421787e-05,
      "loss": 1.5475,
      "step": 1300
    },
    {
      "epoch": 16.883280757097793,
      "grad_norm": 37.48017120361328,
      "learning_rate": 1.1462951234958835e-05,
      "loss": 1.3765,
      "step": 1350
    },
    {
      "epoch": 17.50473186119874,
      "grad_norm": 39.726131439208984,
      "learning_rate": 1.1146295123495884e-05,
      "loss": 1.2068,
      "step": 1400
    },
    {
      "epoch": 18.126182965299684,
      "grad_norm": 28.930688858032227,
      "learning_rate": 1.0829639012032934e-05,
      "loss": 1.0815,
      "step": 1450
    },
    {
      "epoch": 18.757097791798106,
      "grad_norm": 30.35698127746582,
      "learning_rate": 1.0512982900569981e-05,
      "loss": 0.9239,
      "step": 1500
    },
    {
      "epoch": 19.378548895899055,
      "grad_norm": 27.027490615844727,
      "learning_rate": 1.0196326789107031e-05,
      "loss": 0.8767,
      "step": 1550
    },
    {
      "epoch": 20.0,
      "grad_norm": 10.51457405090332,
      "learning_rate": 9.87967067764408e-06,
      "loss": 0.8203,
      "step": 1600
    },
    {
      "epoch": 20.630914826498422,
      "grad_norm": 22.985668182373047,
      "learning_rate": 9.563014566181128e-06,
      "loss": 0.7143,
      "step": 1650
    },
    {
      "epoch": 21.252365930599368,
      "grad_norm": 72.87451171875,
      "learning_rate": 9.246358454718178e-06,
      "loss": 0.5635,
      "step": 1700
    },
    {
      "epoch": 21.883280757097793,
      "grad_norm": 102.89859771728516,
      "learning_rate": 8.929702343255225e-06,
      "loss": 0.5687,
      "step": 1750
    },
    {
      "epoch": 22.50473186119874,
      "grad_norm": 26.492328643798828,
      "learning_rate": 8.613046231792275e-06,
      "loss": 0.4713,
      "step": 1800
    },
    {
      "epoch": 23.126182965299684,
      "grad_norm": 37.411460876464844,
      "learning_rate": 8.296390120329323e-06,
      "loss": 0.4653,
      "step": 1850
    },
    {
      "epoch": 23.757097791798106,
      "grad_norm": 22.083114624023438,
      "learning_rate": 7.979734008866372e-06,
      "loss": 0.3653,
      "step": 1900
    },
    {
      "epoch": 24.378548895899055,
      "grad_norm": 32.579349517822266,
      "learning_rate": 7.66307789740342e-06,
      "loss": 0.3908,
      "step": 1950
    },
    {
      "epoch": 25.0,
      "grad_norm": 2.689736843109131,
      "learning_rate": 7.346421785940469e-06,
      "loss": 0.3225,
      "step": 2000
    },
    {
      "epoch": 25.630914826498422,
      "grad_norm": 20.279434204101562,
      "learning_rate": 7.029765674477518e-06,
      "loss": 0.2828,
      "step": 2050
    },
    {
      "epoch": 26.252365930599368,
      "grad_norm": 12.145851135253906,
      "learning_rate": 6.713109563014567e-06,
      "loss": 0.2618,
      "step": 2100
    },
    {
      "epoch": 26.883280757097793,
      "grad_norm": 19.436939239501953,
      "learning_rate": 6.396453451551615e-06,
      "loss": 0.2239,
      "step": 2150
    },
    {
      "epoch": 27.50473186119874,
      "grad_norm": 18.583505630493164,
      "learning_rate": 6.079797340088665e-06,
      "loss": 0.2127,
      "step": 2200
    },
    {
      "epoch": 28.126182965299684,
      "grad_norm": 4.680973529815674,
      "learning_rate": 5.7631412286257125e-06,
      "loss": 0.2028,
      "step": 2250
    },
    {
      "epoch": 28.757097791798106,
      "grad_norm": 35.23367691040039,
      "learning_rate": 5.446485117162762e-06,
      "loss": 0.1998,
      "step": 2300
    },
    {
      "epoch": 29.378548895899055,
      "grad_norm": 17.776533126831055,
      "learning_rate": 5.12982900569981e-06,
      "loss": 0.1882,
      "step": 2350
    },
    {
      "epoch": 30.0,
      "grad_norm": 9.884483337402344,
      "learning_rate": 4.813172894236859e-06,
      "loss": 0.1815,
      "step": 2400
    },
    {
      "epoch": 30.630914826498422,
      "grad_norm": 4.0308122634887695,
      "learning_rate": 4.496516782773908e-06,
      "loss": 0.159,
      "step": 2450
    },
    {
      "epoch": 31.252365930599368,
      "grad_norm": 7.257286548614502,
      "learning_rate": 4.1798606713109565e-06,
      "loss": 0.1556,
      "step": 2500
    },
    {
      "epoch": 31.883280757097793,
      "grad_norm": 8.521364212036133,
      "learning_rate": 3.863204559848005e-06,
      "loss": 0.1548,
      "step": 2550
    },
    {
      "epoch": 32.504731861198735,
      "grad_norm": 7.54580020904541,
      "learning_rate": 3.5465484483850542e-06,
      "loss": 0.1248,
      "step": 2600
    },
    {
      "epoch": 33.126182965299684,
      "grad_norm": 15.17807674407959,
      "learning_rate": 3.229892336922103e-06,
      "loss": 0.1092,
      "step": 2650
    },
    {
      "epoch": 33.75709779179811,
      "grad_norm": 1.8003284931182861,
      "learning_rate": 2.913236225459152e-06,
      "loss": 0.1377,
      "step": 2700
    },
    {
      "epoch": 34.37854889589905,
      "grad_norm": 2.131706714630127,
      "learning_rate": 2.5965801139962005e-06,
      "loss": 0.1134,
      "step": 2750
    },
    {
      "epoch": 35.0,
      "grad_norm": 2.6075212955474854,
      "learning_rate": 2.279924002533249e-06,
      "loss": 0.1202,
      "step": 2800
    },
    {
      "epoch": 35.630914826498426,
      "grad_norm": 30.462970733642578,
      "learning_rate": 1.963267891070298e-06,
      "loss": 0.1098,
      "step": 2850
    },
    {
      "epoch": 36.25236593059937,
      "grad_norm": 1.8606773614883423,
      "learning_rate": 1.6466117796073467e-06,
      "loss": 0.1135,
      "step": 2900
    },
    {
      "epoch": 36.88328075709779,
      "grad_norm": 10.287884712219238,
      "learning_rate": 1.3299556681443953e-06,
      "loss": 0.0939,
      "step": 2950
    },
    {
      "epoch": 37.504731861198735,
      "grad_norm": 5.596104145050049,
      "learning_rate": 1.0132995566814441e-06,
      "loss": 0.1045,
      "step": 3000
    },
    {
      "epoch": 38.126182965299684,
      "grad_norm": 20.849687576293945,
      "learning_rate": 6.966434452184928e-07,
      "loss": 0.1101,
      "step": 3050
    },
    {
      "epoch": 38.75709779179811,
      "grad_norm": 7.495628833770752,
      "learning_rate": 3.7998733375554155e-07,
      "loss": 0.0913,
      "step": 3100
    },
    {
      "epoch": 39.37854889589905,
      "grad_norm": 14.195818901062012,
      "learning_rate": 6.333122229259026e-08,
      "loss": 0.0869,
      "step": 3150
    }
  ],
  "logging_steps": 50,
  "max_steps": 3160,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.527958830746106e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
